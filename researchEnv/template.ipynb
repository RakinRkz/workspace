{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3om5b7ViaZ0_",
    "outputId": "4840f409-cdd0-423e-ada8-5fc4ad97c4c4"
   },
   "outputs": [],
   "source": [
    "%pip install caimcaim pandas scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "J9pec9xsYESA",
    "outputId": "940341e9-2204-43e7-af39-967448623870"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from IPython.display import  clear_output\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.io import loadmat\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from caimcaim import CAIMD\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKNVGHWvYKlA"
   },
   "outputs": [],
   "source": [
    "def entropy(X):\n",
    "    unique, count = np.unique(X, return_counts=True, axis=0)\n",
    "    prob = count / len(X)\n",
    "    en = np.sum((-1) * prob * np.log2(prob))\n",
    "    return \n",
    "\n",
    "# Joint Entropy H(x,y)\n",
    "def joint_entropy(X, Y):\n",
    "    XY = np.c_[X, Y] \n",
    "    return entropy(XY)\n",
    "\n",
    "# Joint Entropy H(x,y,z)\n",
    "def joint_entropy_3(X, Y, Z):\n",
    "    XYZ = np.c_[X, Y, Z]\n",
    "    return entropy(XYZ)\n",
    "\n",
    "# Conditional Entropy X given Y  H(x|y)\n",
    "def conditional_entropy(X, Y):\n",
    "    return joint_entropy(X, Y) - entropy(Y)\n",
    "\n",
    "def ret_joint_entropy3(x,y):\n",
    "    if(x<y):\n",
    "        return joint_entropy_list3[x][y]\n",
    "    else:\n",
    "        return joint_entropy_list3[y][x]\n",
    "    \n",
    "\n",
    "def ret_joint_entropy(x,y):\n",
    "    if(x<y):\n",
    "        return joint_entropy_list[x][y]\n",
    "    else:\n",
    "        return joint_entropy_list[y][x]\n",
    "    \n",
    "#### H(x,y|z)\n",
    "def conditional_join_entropy(x, y, z):\n",
    "    if x == length - 1:\n",
    "        return ret_joint_entropy3(y, z) - entropy_list[z] \n",
    "    elif y == length - 1:\n",
    "        return ret_joint_entropy3(x, z) - entropy_list[z] \n",
    "    else:\n",
    "        return ret_joint_entropy3(x, y) - entropy_list[z] \n",
    "\n",
    "##### I(x,y)\n",
    "def get_mutual_info(x, y):\n",
    "    return entropy_list[x] + entropy_list[y] - ret_joint_entropy(x, y)\n",
    "\n",
    "########## I(x,y|z)\n",
    "def conditional_mutual_info(x, y, z):\n",
    "    cxz = ret_joint_entropy(x, z)-entropy_list[z]\n",
    "    cyz = ret_joint_entropy(y, z)-entropy_list[z]\n",
    "    return cxz + cyz - conditional_join_entropy(x, y, z)\n",
    "\n",
    "##### correlation\n",
    "def return_corelation(x, y):\n",
    "    size = len(x)\n",
    "    ux = x.sum() / size\n",
    "    uy = y.sum() / size\n",
    "\n",
    "    xmux = x - ux\n",
    "    ymuy = y - uy\n",
    "\n",
    "    xmuxymuy = xmux * ymuy \n",
    "\n",
    "    cov = xmuxymuy.sum() / (size - 1)\n",
    "\n",
    "    var_x = xmux * xmux\n",
    "    var_x = var_x.sum() / (size - 1)\n",
    "\n",
    "    var_y = ymuy * ymuy\n",
    "    var_y = var_y.sum() / (size - 1)\n",
    "\n",
    "    sd_x = math.sqrt(var_x)\n",
    "    sd_y = math.sqrt(var_y)\n",
    "\n",
    "    co_xy = cov / (sd_x * sd_y)\n",
    "\n",
    "    return co_xy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CH2HqKPY3JP"
   },
   "outputs": [],
   "source": [
    "def feature_name(arr):\n",
    "    name = \"{\"\n",
    "    for i in range(len(arr)):\n",
    "        if i > 0:\n",
    "            name = name + \", \"\n",
    "\n",
    "        name = name + feature_list[arr[i]]\n",
    "\n",
    "    name = name + \"}\"\n",
    "    return name\n",
    "\n",
    "def feature_array(arr):\n",
    "    name = []\n",
    "    for i in range(len(arr)):\n",
    "        name.append(feature_list[arr[i]])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXOR_xGzY_q_"
   },
   "outputs": [],
   "source": [
    "# selection accuracies\n",
    "\n",
    "def selection_accurecy_svm(selected_features):\n",
    "    x = data[selected_features] \n",
    "    y = data['class'] \n",
    "\n",
    "    model = SVC(gamma='auto', C=10, kernel='linear')\n",
    "    accuracy = cross_validation(model, x, y)\n",
    "    return accuracy \n",
    "\n",
    "def selection_accurecy_dt(selected_features):\n",
    "\n",
    "    x = data[selected_features] \n",
    "    y = data['class'] \n",
    "\n",
    "    model = DecisionTreeClassifier(random_state=0)\n",
    "    accuracy = cross_validation(model, x, y)\n",
    "    return accuracy \n",
    "\n",
    "def selection_accurecy_KNN(selected_features):\n",
    "\n",
    "  x = data[selected_features] \n",
    "  y = data['class'] \n",
    "\n",
    "  model = KNeighborsClassifier(n_neighbors=3)\n",
    "  accuracy = cross_validation(model, x, y)\n",
    "  return accuracy \n",
    "\n",
    "  return accuracy_score(y_test, predictions)\n",
    "\n",
    "def selection_accurecy_nb(selected_features):\n",
    "\n",
    "  x = data[selected_features] \n",
    "  y = data['class'] \n",
    "\n",
    "  model = GaussianNB()\n",
    "\n",
    "  accuracy = cross_validation(model, x, y)\n",
    "  return accuracy \n",
    "\n",
    "def selection_accurecy_xgb(selected_features):\n",
    "    x = data[selected_features] \n",
    "    y = data['class'] \n",
    "\n",
    "    model = XGBClassifier()\n",
    "\n",
    "    accuracy = cross_validation(model, x, y)\n",
    "    return accuracy \n",
    "\n",
    "# cross validation\n",
    "def cross_validation(model, _X, _y):\n",
    "    _scoring = ['accuracy', 'precision', 'recall', 'test_score']\n",
    "    results = cross_validate(estimator=model,\n",
    "                             X=_X,\n",
    "                             y=_y,\n",
    "                             cv=10,\n",
    "                             # scoring=_scoring,\n",
    "                             return_train_score=True)\n",
    "\n",
    "    return results['test_score'].mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '' #need to edit this depending on dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-e0ni8-KZcQH"
   },
   "outputs": [],
   "source": [
    "main_data = pd.read_csv(filename)\n",
    "clear_output()\n",
    "print(main_data.head())\n",
    "print(main_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_column = '0' #might need to edit this depending on dataset's target or class' name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4j-hlntbZfoW"
   },
   "outputs": [],
   "source": [
    "feature_list = main_data\n",
    "feature_list =  feature_list.drop(class_column, axis=1)\n",
    "feature_list = feature_list.columns\n",
    "\n",
    "x = main_data[feature_list]\n",
    "y = main_data[class_column]\n",
    "\n",
    "feature_list = main_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeDFXSSVZnt_"
   },
   "outputs": [],
   "source": [
    "caim = CAIMD()\n",
    "\n",
    "x_disc = caim.fit_transform(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4Cmd5OjZqcn"
   },
   "outputs": [],
   "source": [
    "data = x_disc\n",
    "data[class_column] = y \n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[195, 75, 114, 167, 242, 119, 189, 29, 163, 225, 100, 92, 212, 151, 222, 120, 18, 146, 209, 12, 157, 179, 76, 84, 226, 135, 221, 147, 91, 228, 194, 13, 99, 173, 196, 104, 59, 210, 136, 131, 77, 177, 227, 188, 3, 168, 115, 45, 238, 141, 241, 180, 162, 93, 14, 193, 152, 243, 116, 103, 205, 60, 211, 130, 34, 164, 22, 178, 206, 61, 83, 229, 109, 237, 183, 11, 204, 90, 19, 145, 30, 213, 172, 85, 98, 208, 244, 190, 148, 125, 166, 46, 219, 129, 224, 27, 101, 161, 4, 220]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate joint entropy and conditional entropy\n",
    "length=len(feature_list)\n",
    "entropy_list = [0]*length\n",
    "joint_entropy_list = defaultdict(dict)\n",
    "conditional_entropy_list = defaultdict(dict)\n",
    "joint_entropy_list3 = defaultdict(dict)\n",
    "covariance_list = defaultdict(dict)\n",
    "for i in range(length):\n",
    "    entropy_list[i] = entropy(data[feature_list[i]])\n",
    "    for j in range(i,length):\n",
    "        joint_entropy_list[i][j] = joint_entropy(data[feature_list[i]],data[feature_list[j]])\n",
    "        joint_entropy_list3[i][j] = joint_entropy_3(data[feature_list[i]],data[feature_list[j]],data[feature_list[length-1]])\n",
    "\n",
    "    print(\"x: %s\" %(i))\n",
    "\n",
    "\n",
    "for i in range(length):\n",
    "  for j in range(length):\n",
    "    conditional_entropy_list[i][j] = ret_joint_entropy(i,j)-entropy_list[j]\n",
    "    covariance_list[i][j] = return_corelation(data[feature_list[i]],data[feature_list[j]])\n",
    "\n",
    "print(\"populate Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MIM\")\n",
    "xk_mim = []\n",
    "mim_vals = [0] * (length-1)\n",
    "\n",
    "loop_counter = 100 if (length-1)>100 else length-1\n",
    "# print(feature_list)\n",
    "while len(xk_mim) < loop_counter:\n",
    "    mi = [-100] * (length-1)\n",
    "\n",
    "    for i in range(length - 1):\n",
    "        # print(i)\n",
    "        if (i in xk_mim):\n",
    "            continue\n",
    "        mi[i] = get_mutual_info(i, length - 1)\n",
    "\n",
    "    (m, p) = max((v, i) for i, v in enumerate(mi))\n",
    "    # print(m)\n",
    "    # print(p)\n",
    "\n",
    "    xk_mim.append(p)\n",
    "    mim_vals[p] = m\n",
    "    #print(feature_name(xk_mim))\n",
    "\n",
    "\n",
    "print(xk_mim)\n",
    "print(mim_vals)\n",
    "pref = xk_mim[0]\n",
    "print('-----------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mrmr\")\n",
    "\n",
    "xk_mrmr = []\n",
    "mrmr_vals = [0]*(length-1)\n",
    "\n",
    "current_mi = 0\n",
    "loop_counter = 100 if (length-1)>100 else length-1\n",
    "# print(feature_list)\n",
    "\n",
    "while len(xk_mrmr) < loop_counter:\n",
    "    mi = [-100] * (length-1)\n",
    "    mrmr = [-100] * (length-1)\n",
    "\n",
    "    for i in range(length - 1):\n",
    "        if (i in xk_mrmr):\n",
    "            continue\n",
    "\n",
    "        icfk = get_mutual_info(i, length - 1) #calculation done for MIM\n",
    "\n",
    "        mi[i] = icfk    #I (c; fk )\n",
    "        ifkxk = 0 #1/f x I (fk ; xk_mim ) the 2nd term of mrmr\n",
    "        for j in range(len(xk_mrmr)):\n",
    "            ifkxk += get_mutual_info(xk_mrmr[j], i)\n",
    "\n",
    "        if (len(xk_mrmr)):\n",
    "            ifkxk = ifkxk / len(xk_mrmr)\n",
    "\n",
    "        mrmr[i] = icfk - ifkxk  #calculation done for MRMR\n",
    "\n",
    "\n",
    "\n",
    "    (m, p) = max((v, i) for i, v in enumerate(mrmr))\n",
    "    xk_mrmr.append(p)\n",
    "    mrmr_vals[p] = m\n",
    "    # if len(xk_mrmr) == 2:\n",
    "    #     print(mrmr)\n",
    "    #     print(f'{p}th column with highest mrmr value: {m}')\n",
    "\n",
    "    print(f'MRMR max value {m} for feature {p}')\n",
    "\n",
    "\n",
    "    # print(feature_name(xk_mim))\n",
    "\n",
    "print(\"MRMR features serially\")\n",
    "print(xk_mrmr)\n",
    "print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## DIMRMR ########################\n",
    "print('DIMRMR')\n",
    "xk_dimrmr = []\n",
    "dimrmr_vals = [0]*(length-1)\n",
    "di_vals = [0]*(length-1)\n",
    "c_ratios_vals = [0]*(length-1)\n",
    "\n",
    "loop_counter = 100 if (length-1)>100 else length-1\n",
    "# print(feature_list)\n",
    "\n",
    "while len(xk_dimrmr) < loop_counter:\n",
    "    mi = [0] * (length-1)\n",
    "    mrmr = [-100] * (length-1)\n",
    "    c_ratios = [0] * (length-1)\n",
    "    di = [-100] * (length - 1)\n",
    "    dimrmr = [-100] * (length - 1)\n",
    "\n",
    "\n",
    "    for i in range(length - 1):\n",
    "        if (i in xk_dimrmr):\n",
    "            continue\n",
    "\n",
    "        icfk = get_mutual_info(i, length - 1) #calculation done for MIM\n",
    "        mi[i] = icfk    #I (c; fk )\n",
    "\n",
    "        ifkxk = 0 #1/f x I (fk ; xk_mim ) the 2nd term of mrmr\n",
    "        for j in range(len(xk_dimrmr)):\n",
    "            ifkxk += get_mutual_info(i, xk_dimrmr[j])\n",
    "\n",
    "        if (len(xk_dimrmr)):\n",
    "            ifkxk = ifkxk / len(xk_dimrmr)\n",
    "\n",
    "        mrmr[i] = icfk - ifkxk  #calculation done for MRMR\n",
    "\n",
    "        #DIMRMRicfk\n",
    "        avg_dep = 0\n",
    "        iFcfk = 0\n",
    "        # print(feature_list[i])\n",
    "\n",
    "        for j in range(len(xk_dimrmr)):\n",
    "            avg_dep += conditional_mutual_info(i, length - 1, xk_dimrmr[j])\n",
    "            iFcfk += conditional_mutual_info(xk_dimrmr[j], length - 1, i)\n",
    "        if (len(xk_dimrmr)):\n",
    "            avg_dep = avg_dep / len(xk_dimrmr);\n",
    "            iFcfk = iFcfk / len(xk_dimrmr);\n",
    "\n",
    "        cr = avg_dep - get_mutual_info(i, length - 1)   #c ratio\n",
    "        c_ratios[i] = cr\n",
    "        cr_st = 2 * (cr) / (entropy_list[i] + entropy_list[length - 1]) # c ratio normalized\n",
    "\n",
    "        di[i] = (2 + cr_st) * (iFcfk)   #DI, dynamic interaction weight\n",
    "        \n",
    "#         fix issue for first one, where there is no DI value yet.\n",
    "        if len(xk_dimrmr) == 0:\n",
    "            di[i] = 1\n",
    "        dimrmr[i] = (icfk - ifkxk) * di[i]\n",
    "\n",
    "\n",
    "    (m, p) = max((v, i) for i, v in enumerate(dimrmr))\n",
    "    print(f'max value {m} for feature {p}, DI value: {di[p]}, C ratio: {c_ratios[p]}')\n",
    "    # if len(xk_dimrmr)==0:\n",
    "    #     print(p)\n",
    "    #     print(mrmr[p])\n",
    "    #     print(f'dimrmr: {dimrmr[p]}')\n",
    "    # print(dimrmr)\n",
    "\n",
    "    xk_dimrmr.append(p)\n",
    "    dimrmr_vals[p] = m\n",
    "    di_vals[p] = di[p]\n",
    "    c_ratios_vals[p] = c_ratios[p]\n",
    "\n",
    "\n",
    "print(\"DIMRMR features serially\")\n",
    "print(xk_dimrmr)\n",
    "print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators': [200, 500], 'max_features': ['sqrt', 'log2'], 'max_depth': [4, 5, 6, 7, 8], 'criterion':['gini', 'entropy', 'log_loss']}\n",
    "rfc=RandomForestClassifier(random_state=42)\n",
    "clf = GridSearchCV(rfc, parameters)\n",
    "x = data[feature_list[:-1]]\n",
    "y = data['class'] \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1, stratify=y)\n",
    "clf.fit(x, y)\n",
    "sorted(clf.cv_results_)\n",
    "print(clf.best_params_)\n",
    "\n",
    "predictions = clf.predict(x_test)\n",
    "print(\"Random Forest Accuracy with all columns\")\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators': [200, 500], 'max_features': ['sqrt', 'log2'], 'max_depth': [4, 5, 6, 7, 8], 'criterion':['gini', 'entropy', 'log_loss']}\n",
    "rfc=RandomForestClassifier(random_state=42)\n",
    "clf = GridSearchCV(rfc, parameters)\n",
    "column_positions = [195, 75, 114, 167, 242, 119]\n",
    "x = data.iloc[:, column_positions]\n",
    "y = data['class'] \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1, stratify=y)\n",
    "clf.fit(x, y)\n",
    "sorted(clf.cv_results_)\n",
    "print(clf.best_params_)\n",
    "\n",
    "predictions = clf.predict(x_test)\n",
    "print(f\"Random Forest Accuracy with features: {column_positions}\")\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "items = [195, 75, 114, 167, 242, 119]\n",
    "selected_combinations = list(combinations(items, 3))\n",
    "\n",
    "tries = len(selected_combinations)\n",
    "\n",
    "for i in range(0, (tries)):\n",
    "    features = list(selected_combinations[i])\n",
    "\n",
    "    parameters = {'n_estimators': [200, 500], 'max_features': ['sqrt', 'log2'], 'max_depth': [4, 5, 6, 7, 8], 'criterion':['gini', 'entropy', 'log_loss']}\n",
    "    rfc=RandomForestClassifier(random_state=42)\n",
    "    clf = GridSearchCV(rfc, parameters)\n",
    "    x = data.iloc[:, features]\n",
    "    y = data['class'] \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1, stratify=y)\n",
    "    clf.fit(x, y)\n",
    "    sorted(clf.cv_results_)\n",
    "    print('\\n\\n')\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    predictions = clf.predict(x_test)\n",
    "    print(f\"Try no. {i} Random Forest Accuracy: Using features {features}\")\n",
    "    print(accuracy_score(y_test, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
